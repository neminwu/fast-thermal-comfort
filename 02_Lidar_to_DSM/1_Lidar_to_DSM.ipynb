{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b501ab71",
   "metadata": {},
   "source": [
    "# 2. Lidar to DSM\n",
    "\n",
    "This function is to convert lidar (.laz) file into tiff (.tif) format that we can use for processing next steps. You will need this libraries for running lidar to DSM.\n",
    "\n",
    "If you don't have any of these libraries, I recommend using 'conda install -c conda-forge {library}' to install library. I recommend using conda-forge as priority channel to make sure to meet all dependencies for each library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import laspy\n",
    "import glob\n",
    "import os\n",
    "import pdal\n",
    "import subprocess\n",
    "import traceback\n",
    "from osgeo import gdal, osr \n",
    "import math \n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7d4a0",
   "metadata": {},
   "source": [
    "## 2.1. Filter Classes \n",
    "\n",
    "This function filters LAZ points based on lidar classification codes. Please refer to Github Wiki for classification number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06024953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import laspy\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "### --- Configuration (User MUST update these paths) --- ###\n",
    "LIDAR_DIR = \"/media/remap/NO_HEAT_RB/City_Atlanta/Raw/LiDAR/\"\n",
    "FILTERED_LIDAR_DIR = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/Test_classify/\"\n",
    "FILTER_CLASS = [2,6,9,17] # ground, building, water, bridge deck\n",
    "### ---------------------------------------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f25cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lidar_by_classification(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    classification_filter: list\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Filters LAZ files based on specified classification codes and writes the filtered outputs.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir (str): Directory containing the input LAZ files.\n",
    "    - output_dir (str): Directory where filtered LAZ files will be saved.\n",
    "    - classification_filter (list): List of classification codes to retain (e.g., [2, 6, 9, 17]).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    laz_files = glob.glob(os.path.join(input_dir, \"*.laz\"))\n",
    "\n",
    "    for file_path in laz_files:\n",
    "        try:\n",
    "            las = laspy.read(file_path)\n",
    "            mask = np.isin(las.classification, classification_filter)\n",
    "            filtered_las = las[mask]\n",
    "\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            output_file = os.path.join(output_dir, f\"{base_name}_filtered.laz\")\n",
    "\n",
    "            filtered_las.write(output_file)\n",
    "            print(f\"Filtered_done for {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filter_lidar_by_classification(LIDAR_DIR, FILTERED_LIDAR_DIR, FILTER_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb20ae",
   "metadata": {},
   "source": [
    "## 2.2. Merge LiDAR\n",
    "\n",
    "This function merges all .laz files in the input directory into singe LAZ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74810778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "from osgeo import gdal\n",
    "import time\n",
    "import os\n",
    "\n",
    "# os.environ['PROJ_DATA'] = '/home/hyu483/.conda/envs/remap/share/proj' # Only use this when python cannot find 'PROJ_DATA' location.\n",
    "\n",
    "\n",
    "### --- Configuration (User MUST update these paths) --- ###\n",
    "LIDAR_TO_MERGE = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/test_classify/\" \n",
    "LIDAR_MERGED = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/Lidar_merged.laz\"\n",
    "### ---------------------------------------------------- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8dbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_laz_files(input_dir: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Merges all .laz files in the input directory into a single LAZ file using PDAL.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir (str): Directory containing the input LAZ files.\n",
    "    - output_file (str): Path to the output merged LAZ file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Ensure input directory exists\n",
    "    if not os.path.exists(input_dir):\n",
    "        raise FileNotFoundError(f\"Input directory does not exist: {input_dir}\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    laz_files = os.path.join(os.path.join(input_dir, \"*.laz\"))\n",
    "    print(laz_files)\n",
    "    if not laz_files:\n",
    "        print(f\"No LAZ files found in: {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Create the pipeline:\n",
    "    # 1. Read all .laz files matching the pattern.\n",
    "    # 2. Merge them.\n",
    "    # 3. Write the merged output to a new file.\n",
    "    pipeline_merge = (\n",
    "        pdal.Reader.las(filename=laz_files)\n",
    "        | pdal.Filter.merge()\n",
    "        | pdal.Filter.voxeldownsize(\n",
    "            cell=0.1,        \n",
    "            mode=\"center\"     \n",
    "        )\n",
    "        | pdal.Writer.las(filename=output_file)\n",
    "    )\n",
    "\n",
    "    print(\"Executing pipeline...\")\n",
    "\n",
    "    try:\n",
    "        pipeline_merge.execute()\n",
    "        print(\"File read successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error reading LAZ file:\", e)\n",
    "\n",
    "    print(f\"Merged .laz files saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b60818",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    merge_laz_files(LIDAR_TO_MERGE, LIDAR_MERGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9407d",
   "metadata": {},
   "source": [
    "## 2.3. Tiling, Rasterization, Merge LiDAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954804db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import os\n",
    "import subprocess\n",
    "import traceback\n",
    "from osgeo import gdal, osr \n",
    "import json \n",
    "\n",
    "### --- Configuration (User MUST update these paths) --- ###\n",
    "INPUT_LIDAR = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/merged.laz\"\n",
    "BUFFERED_TILES_DIR = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/tiles_buffered\"\n",
    "INTERMEDIATE_DEBUFFERED_DIR = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/tiles_debuffered\"\n",
    "OUTPUT_TIF_PATH = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/DSM/DSM_merged.tif\"\n",
    "\n",
    "# Tiling and Rasterization parameters\n",
    "TILE_LENGTH = 1000.0 \n",
    "BUFFER = 20.0        \n",
    "RESOLUTION = 1.0     \n",
    "OUTPUT_TYPE = \"mean\"\n",
    "DIM = \"Z\"      \n",
    "NODATA = -9999.0     \n",
    "### --- Configuration (User MUST update these paths) --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68abfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gdal_sub_geotransform(parent_gt, x_offset_pixels, y_offset_pixels):\n",
    "    \"\"\"\n",
    "    Calculates the geotransform for a sub-region (subset) of a raster.\n",
    "    This is crucial for correctly georeferencing the debuffered tiles.\n",
    "\n",
    "    Args:\n",
    "        parent_gt (tuple): The geotransform of the parent raster.\n",
    "                           Format: (top_left_x, pixel_width, row_rotation_x, top_left_y, col_rotation_y, pixel_height).\n",
    "        x_offset_pixels (int): X-offset (column offset) of the sub-region's top-left corner\n",
    "                               relative to the parent's top-left corner, in pixels.\n",
    "        y_offset_pixels (int): Y-offset (row offset) of the sub-region's top-left corner\n",
    "                               relative to the parent's top-left corner, in pixels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The new geotransform for the sub-region.\n",
    "    \"\"\"\n",
    "    # Calculate the new top-left X and Y coordinates based on the parent's geotransform\n",
    "    # and the pixel offsets.\n",
    "    new_top_left_x = parent_gt[0] + x_offset_pixels * parent_gt[1] + y_offset_pixels * parent_gt[2]\n",
    "    new_top_left_y = parent_gt[3] + x_offset_pixels * parent_gt[4] + y_offset_pixels * parent_gt[5]\n",
    "\n",
    "    # The pixel size and rotation components remain the same as the parent raster.\n",
    "    return (new_top_left_x, parent_gt[1], parent_gt[2], new_top_left_y, parent_gt[4], parent_gt[5])\n",
    "\n",
    "\n",
    "def tile_and_rasterize_lidar(input_laz_file, output_dir, tile_length, buffer, resolution, output_type, dimension, nodata, origin_x=None, origin_y=None):\n",
    "    \"\"\"\n",
    "    Tiles a LiDAR .laz file with a buffer using PDAL's filters.splitter,\n",
    "    and then rasterizes each tile to a GeoTIFF using writers.gdal.\n",
    "    These output tiles will include the buffer around their core area.\n",
    "\n",
    "    Args:\n",
    "        input_laz_file (str): Path to the input .laz file.\n",
    "        output_dir (str): Directory to save the output GeoTIFF tiles (these will be buffered).\n",
    "        tile_length (float): Side length of the square tiles (e.g., 1000.0 for 1km x 1km tiles).\n",
    "        buffer (float): Amount of overlap to include in each tile (in ground units).\n",
    "        resolution (float): Resolution of the output raster (in ground units).\n",
    "        output_type (str): Aggregation method for rasterization (e.g., \"mean\", \"min\", \"max\").\n",
    "        dimension (str): The point dimension to use for rasterization (e.g., \"Z\").\n",
    "        nodata (float): NoData value for raster cells with no points.\n",
    "        origin_x (float, optional): X origin for the tiling grid. If None, PDAL determines it.\n",
    "        origin_y (float, optional): Y origin for the tiling grid. If None, PDAL determines it.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the process was successful, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory for buffered tiles: {output_dir}\")\n",
    "\n",
    "    # Define the PDAL pipeline as a Python dictionary.\n",
    "    # 1. Reads the LAZ file\n",
    "    # 2. Splits it into buffered tiles\n",
    "    # 3. then writes each tile as a GeoTIFF.\n",
    "    pipeline_definition = [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": input_laz_file\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"filters.splitter\",\n",
    "            \"length\": tile_length, # Defines the side length of the core tile\n",
    "            \"buffer\": buffer      # Defines the buffer around the core tile\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"writers.gdal\",\n",
    "            # The '#' in the filename is a placeholder for the tile index,\n",
    "            # allowing PDAL to create multiple output files.\n",
    "            \"filename\": os.path.join(output_dir, \"atlanta_dem_buffered_tile_#.tif\"),\n",
    "            \"gdaldriver\": \"GTiff\",\n",
    "            \"resolution\": resolution,\n",
    "            \"output_type\": output_type,\n",
    "            \"dimension\": dimension,\n",
    "            \"nodata\": nodata\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Add origin_x and origin_y to the splitter filter if provided\n",
    "    if origin_x is not None:\n",
    "        pipeline_definition[1][\"origin_x\"] = origin_x\n",
    "    if origin_y is not None:\n",
    "        pipeline_definition[1][\"origin_y\"] = origin_y\n",
    "\n",
    "    # Create a PDAL Pipeline object from the definition\n",
    "    pipeline_json = json.dumps(pipeline_definition)\n",
    "\n",
    "    pipeline = pdal.Pipeline(pipeline_json)\n",
    "    \n",
    "    print(f\"Executing PDAL pipeline for {input_laz_file} to create buffered tiles...\")\n",
    "    try:\n",
    "        # Execute the pipeline and get the number of points processed\n",
    "        count = pipeline.execute()\n",
    "        print(f\"Successfully processed {count} points and created buffered tiles.\")\n",
    "        print(f\"Buffered raster tiles saved to: {output_dir}\")\n",
    "        return True\n",
    "    except pdal.PDALError as e:\n",
    "        print(f\"PDAL Error during tiling and rasterization: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during tiling and rasterization: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def debuffer_and_save_gdal_tile(\n",
    "    buffered_raster_path: str,\n",
    "    debuffered_raster_path: str,\n",
    "    actual_buffer_on_left_pixels: int,\n",
    "    actual_buffer_on_top_pixels: int,\n",
    "    core_tile_width_pixels: int,\n",
    "    core_tile_height_pixels: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Clips the core (non-buffered) data from a buffered raster using GDAL\n",
    "    and saves it with correct global georeferencing.\n",
    "\n",
    "    Args:\n",
    "        buffered_raster_path (str): Path to the input buffered GeoTIFF tile.\n",
    "        debuffered_raster_path (str): Path to save the output debuffered GeoTIFF tile.\n",
    "        actual_buffer_on_left_pixels (int): Number of buffer pixels on the left side of the buffered raster.\n",
    "        actual_buffer_on_top_pixels (int): Number of buffer pixels on the top side of the buffered raster.\n",
    "        core_tile_width_pixels (int): Expected width of the core (debuffered) tile in pixels.\n",
    "        core_tile_height_pixels (int): Expected height of the core (debuffered) tile in pixels.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Path to the debuffered tile if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    # Open the buffered source raster in read-only mode\n",
    "    src_buffered_ds = gdal.Open(buffered_raster_path, gdal.GA_ReadOnly)\n",
    "    if src_buffered_ds is None:\n",
    "        print(f\"ERROR: Could not open buffered raster: {buffered_raster_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Get the first raster band (assuming single-band DEM)\n",
    "        src_buffered_band = src_buffered_ds.GetRasterBand(1)\n",
    "        if src_buffered_band is None:\n",
    "            print(f\"ERROR: Could not get band from {buffered_raster_path}\")\n",
    "            src_buffered_ds = None # Close dataset\n",
    "            return None\n",
    "\n",
    "        # Retrieve geotransform, projection, NoData value, and data type from the buffered source\n",
    "        buffered_gt = src_buffered_ds.GetGeoTransform()\n",
    "        buffered_proj = src_buffered_ds.GetProjection()\n",
    "        no_data_value = src_buffered_band.GetNoDataValue()\n",
    "        gdal_data_type = src_buffered_band.DataType\n",
    "\n",
    "        # Read the core data from the buffered raster.\n",
    "        # xoff, yoff define the top-left pixel of the window to read.\n",
    "        # win_xsize, win_ysize define the width and height of the window to read.\n",
    "        core_data = src_buffered_band.ReadAsArray(\n",
    "            xoff=actual_buffer_on_left_pixels,\n",
    "            yoff=actual_buffer_on_top_pixels,\n",
    "            win_xsize=core_tile_width_pixels,\n",
    "            win_ysize=core_tile_height_pixels\n",
    "        )\n",
    "\n",
    "        if core_data is None:\n",
    "            print(f\"ERROR: Failed to read core data from {buffered_raster_path}\")\n",
    "            src_buffered_ds = None\n",
    "            return None\n",
    "\n",
    "        # Optional: Check if the read data shape matches expected core dimensions.\n",
    "        # This helps in debugging if buffer/tile calculations are slightly off,\n",
    "        # especially for edge tiles where buffers might be truncated.\n",
    "        if core_data.shape[0] != core_tile_height_pixels or core_data.shape[1] != core_tile_width_pixels:\n",
    "            print(f\"WARNING: Read core data shape ({core_data.shape}) does not match expected ({core_tile_height_pixels}, {core_tile_width_pixels}) for {debuffered_raster_path}. This might indicate issues with buffer/tile size calculations or edge tiles.\")\n",
    "            core_tile_height_pixels = core_data.shape[0]\n",
    "            core_tile_width_pixels = core_data.shape[1]\n",
    "\n",
    "        # Calculate the correct geotransform for this debuffered (core) tile.\n",
    "        # This transform ensures the debuffered tile is placed correctly in global coordinates.\n",
    "        final_core_geotransform = calculate_gdal_sub_geotransform(\n",
    "            buffered_gt,\n",
    "            actual_buffer_on_left_pixels,\n",
    "            actual_buffer_on_top_pixels\n",
    "        )\n",
    "\n",
    "        # Get the GDAL driver for GeoTIFF\n",
    "        driver = gdal.GetDriverByName(\"GTiff\")\n",
    "        if driver is None:\n",
    "            print(\"ERROR: GTiff driver not available.\")\n",
    "            src_buffered_ds = None\n",
    "            return None\n",
    "\n",
    "        # Create the output directory for debuffered tiles if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(debuffered_raster_path), exist_ok=True)\n",
    "\n",
    "        # Create the new debuffered raster dataset\n",
    "        dst_ds = driver.Create(\n",
    "            debuffered_raster_path,\n",
    "            xsize=core_tile_width_pixels,\n",
    "            ysize=core_tile_height_pixels,\n",
    "            bands=1, \n",
    "            eType=gdal_data_type, # Use the same data type as the source\n",
    "            options=[\"COMPRESS=LZW\"] # Add LZW compression to the output GeoTIFF\n",
    "        )\n",
    "        if dst_ds is None:\n",
    "            print(f\"ERROR: Could not create output raster: {debuffered_raster_path}\")\n",
    "            src_buffered_ds = None\n",
    "            return None\n",
    "\n",
    "        # Set the geotransform and projection for the new debuffered raster\n",
    "        dst_ds.SetGeoTransform(final_core_geotransform)\n",
    "        dst_ds.SetProjection(buffered_proj) # Preserve the Coordinate Reference System (CRS)\n",
    "\n",
    "        # Write the extracted core data to the new raster band\n",
    "        dst_band = dst_ds.GetRasterBand(1)\n",
    "        dst_band.WriteArray(core_data)\n",
    "        # Set the NoData value if it exists in the source\n",
    "        if no_data_value is not None:\n",
    "            dst_band.SetNoDataValue(no_data_value)\n",
    "\n",
    "        # Flush the cache and close the destination dataset to ensure data is written to disk\n",
    "        dst_band.FlushCache()\n",
    "        dst_ds = None # Closing the dataset saves it\n",
    "\n",
    "        print(f\"Successfully debuffered: {buffered_raster_path} -> {debuffered_raster_path}\")\n",
    "        return debuffered_raster_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during debuffering for {buffered_raster_path} to {debuffered_raster_path}: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Ensure the source buffered dataset is closed\n",
    "        if src_buffered_ds:\n",
    "            src_buffered_ds = None\n",
    "\n",
    "\n",
    "def merge_gdal_tiles(input_tile_dir, output_vrt_name=\"merged_dem.vrt\", output_tif_path=OUTPUT_TIF_PATH, tile_prefix=\"atlanta_dem_debuffered_tile_\"):\n",
    "    \"\"\"\n",
    "    Merges GeoTIFF tiles into a single GeoTIFF using GDAL's command-line utilities.\n",
    "    It first creates a Virtual Raster (VRT) and then translates it to a final GeoTIFF.\n",
    "\n",
    "    Args:\n",
    "        input_tile_dir (str): Directory containing the GeoTIFF tiles to be merged.\n",
    "        output_vrt_name (str): Name for the intermediate Virtual Raster (VRT) file.\n",
    "        output_tif_name (str): Name for the final merged GeoTIFF file.\n",
    "        tile_prefix (str): The prefix of the tile filenames to identify them (e.g., \"atlanta_dem_debuffered_tile_\").\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the merge was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Merging tiles using GDAL ---\")\n",
    "\n",
    "    # Construct the full paths for the VRT and final TIF files\n",
    "    output_vrt_path = os.path.join(input_tile_dir, output_vrt_name)\n",
    "\n",
    "    # Find all tile files in the input directory that match the specified prefix and extension\n",
    "    tile_files = [os.path.join(input_tile_dir, f) for f in os.listdir(input_tile_dir) if f.startswith(tile_prefix) and f.endswith(\".tif\")]\n",
    "\n",
    "    if not tile_files:\n",
    "        print(f\"No tiles found in {input_tile_dir} with prefix '{tile_prefix}'. Skipping merge.\")\n",
    "        return False\n",
    "\n",
    "    # 1: Create a VRT (Virtual Raster) from the tiles\n",
    "    # gdalbuildvrt command: gdalbuildvrt <output_vrt> <input_files...>\n",
    "    gdalbuildvrt_command = [\n",
    "        \"gdalbuildvrt\",\n",
    "        output_vrt_path,\n",
    "    ] + tile_files # Append the list of found tile files to the command\n",
    "\n",
    "    print(f\"Executing: {' '.join(gdalbuildvrt_command)}\")\n",
    "    try:\n",
    "        # Run the gdalbuildvrt command. check=True raises an exception for non-zero exit codes.\n",
    "        subprocess.run(gdalbuildvrt_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully created VRT: {output_vrt_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error creating VRT: {e}\")\n",
    "        print(f\"STDOUT: {e.stdout}\")\n",
    "        print(f\"STDERR: {e.stderr}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'gdalbuildvrt' command not found. Make sure GDAL is installed and in your system's PATH.\")\n",
    "        return False\n",
    "\n",
    "    # 2: Convert the VRT to a single GeoTIFF\n",
    "    # gdal_translate command: gdal_translate <vrt_file> <output_tif_file>\n",
    "    gdal_translate_command = [\n",
    "        \"gdal_translate\",\n",
    "        output_vrt_path,\n",
    "        output_tif_path,\n",
    "        \"-co\", \"COMPRESS=LZW\" # Add LZW compression to the final output GeoTIFF\n",
    "    ]\n",
    "    print(f\"Executing: {' '.join(gdal_translate_command)}\")\n",
    "    try:\n",
    "        # Run the gdal_translate command\n",
    "        subprocess.run(gdal_translate_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully created merged GeoTIFF: {output_tif_path}\")\n",
    "        # Remove the temporary VRT file after successful merging\n",
    "        os.remove(output_vrt_path)\n",
    "        print(f\"Removed temporary VRT: {output_vrt_path}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error converting VRT to GeoTIFF: {e}\")\n",
    "        print(f\"STDOUT: {e.stdout}\")\n",
    "        print(f\"STDERR: {e.stderr}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'gdal_translate' command not found. Make sure GDAL is installed and in your system's PATH.\")\n",
    "        return False\n",
    "\n",
    "def reproject_raster(\n",
    "    input_raster: str,\n",
    "    output_raster: str,\n",
    "    source_epsg: int = 6350,\n",
    "    target_epsg: int = 6446,\n",
    "    resampling: str = \"near\",\n",
    "    output_format: str = \"GTiff\",\n",
    "    overwrite: bool = True\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Reprojects a raster from one EPSG CRS to another using gdalwarp.\n",
    "\n",
    "    Parameters:\n",
    "    - input_raster (str): Path to the input raster file.\n",
    "    - output_raster (str): Path to the output raster file.\n",
    "    - source_epsg (int): EPSG code for the source CRS (default: 6350).\n",
    "    - target_epsg (int): EPSG code for the target CRS (default: 6446).\n",
    "    - resampling (str): Resampling method (default: \"near\").\n",
    "    - output_format (str): Output format (default: \"GTiff\").\n",
    "    - overwrite (bool): Whether to overwrite the output file if it exists.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_raster):\n",
    "        raise FileNotFoundError(f\"Input raster does not exist: {input_raster}\")\n",
    "\n",
    "    if not overwrite and os.path.exists(output_raster):\n",
    "        raise FileExistsError(f\"Output raster already exists and overwrite=False: {output_raster}\")\n",
    "\n",
    "    cmd = [\n",
    "        \"gdalwarp\",\n",
    "        \"-r\", resampling,\n",
    "        \"-s_srs\", f\"EPSG:{source_epsg}\",\n",
    "        \"-t_srs\", f\"EPSG:{target_epsg}\",\n",
    "        \"-of\", output_format,\n",
    "    ]\n",
    "\n",
    "    if overwrite:\n",
    "        cmd.append(\"-overwrite\")\n",
    "\n",
    "    cmd.extend([input_raster, output_raster])\n",
    "\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"Reprojection complete: {output_raster}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during reprojection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bb02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Ensure output directories exist before starting the process\n",
    "    os.makedirs(BUFFERED_TILES_DIR, exist_ok=True)\n",
    "    os.makedirs(INTERMEDIATE_DEBUFFERED_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Step 1: Tile LAZ and Rasterize to Buffered GeoTIFFs ---\n",
    "    print(\"\\n--- Step 1: Tiling LAZ and Rasterizing to Buffered GeoTIFFs ---\")\n",
    "    tiling_successful = tile_and_rasterize_lidar(\n",
    "        input_laz_file=INPUT_LIDAR,\n",
    "        output_dir=BUFFERED_TILES_DIR, # Buffered tiles will be saved here\n",
    "        tile_length=TILE_LENGTH,\n",
    "        buffer=BUFFER,\n",
    "        resolution=RESOLUTION,\n",
    "        output_type=OUTPUT_TYPE,\n",
    "        dimension=DIM,\n",
    "        nodata=NODATA\n",
    "    )\n",
    "\n",
    "    if tiling_successful:\n",
    "        # --- Step 2: Debuffer each GeoTIFF tile ---\n",
    "        print(\"\\n--- Step 2: Debuffering each GeoTIFF tile ---\")\n",
    "        # Define prefixes for the buffered and debuffered tiles for easy identification\n",
    "        buffered_tile_prefix = \"atlanta_dem_buffered_tile_\" # Matches the filename used in writers.gdal\n",
    "        debuffered_tile_prefix = \"atlanta_dem_debuffered_tile_\" # New prefix for the debuffered tiles\n",
    "\n",
    "        # Calculate buffer and core tile dimensions in pixels based on ground units and resolution\n",
    "        buffer_pixels = int(BUFFER / RESOLUTION)\n",
    "        core_tile_width_pixels = int(TILE_LENGTH / RESOLUTION)\n",
    "        core_tile_height_pixels = int(TILE_LENGTH / RESOLUTION)\n",
    "\n",
    "        debuffering_successful_count = 0\n",
    "        total_buffered_tiles = 0\n",
    "\n",
    "        # Iterate through all files in the directory where buffered tiles were saved\n",
    "        for filename in os.listdir(BUFFERED_TILES_DIR):\n",
    "            # Check if the file is a buffered GeoTIFF tile\n",
    "            if filename.startswith(buffered_tile_prefix) and filename.endswith(\".tif\"):\n",
    "                total_buffered_tiles += 1\n",
    "                buffered_tile_path = os.path.join(BUFFERED_TILES_DIR, filename)\n",
    "\n",
    "                # Construct the new filename for the debuffered tile\n",
    "                debuffered_filename = filename.replace(buffered_tile_prefix, debuffered_tile_prefix)\n",
    "                debuffered_tile_path = os.path.join(INTERMEDIATE_DEBUFFERED_DIR, debuffered_filename)\n",
    "\n",
    "                # Call the debuffering function for the current tile\n",
    "                debuffered_result = debuffer_and_save_gdal_tile(\n",
    "                    buffered_raster_path=buffered_tile_path,\n",
    "                    debuffered_raster_path=debuffered_tile_path,\n",
    "                    actual_buffer_on_left_pixels=buffer_pixels,\n",
    "                    actual_buffer_on_top_pixels=buffer_pixels,\n",
    "                    core_tile_width_pixels=core_tile_width_pixels,\n",
    "                    core_tile_height_pixels=core_tile_height_pixels\n",
    "                )\n",
    "                if debuffered_result:\n",
    "                    debuffering_successful_count += 1\n",
    "                else:\n",
    "                    print(f\"Failed to debuffer tile: {buffered_tile_path}\")\n",
    "\n",
    "        # Check the overall success of the debuffering step\n",
    "        if total_buffered_tiles > 0 and debuffering_successful_count == total_buffered_tiles:\n",
    "            print(f\"Successfully debuffered all {debuffering_successful_count} tiles.\")\n",
    "            debuffering_overall_successful = True\n",
    "        elif total_buffered_tiles == 0:\n",
    "            print(\"No buffered tiles found to debuffer. Skipping debuffering and merging.\")\n",
    "            debuffering_overall_successful = False\n",
    "        else:\n",
    "            print(f\"Completed debuffering with {debuffering_successful_count}/{total_buffered_tiles} tiles successfully processed. Please check the logs above for specific errors.\")\n",
    "            debuffering_overall_successful = True # Indicate partial or full failure\n",
    "\n",
    "        # --- Step 3: Merge the debuffered tiles ---\n",
    "\n",
    "        if debuffering_overall_successful:\n",
    "            print(\"\\n--- Step 3: Merging debuffered tiles ---\")\n",
    "            merge_gdal_tiles(\n",
    "                input_tile_dir=INTERMEDIATE_DEBUFFERED_DIR, # Merge tiles from the debuffered directory\n",
    "                output_vrt_name=\"atlanta_merged_dem.vrt\", # Custom VRT name for the final merge\n",
    "                output_tif_path=OUTPUT_TIF_PATH, # Custom TIF name for the final merged output\n",
    "                tile_prefix=debuffered_tile_prefix # Specify the prefix for the debuffered tiles to merge\n",
    "            )\n",
    "        \n",
    "        # --- Step 4: Reproject the output file ---\n",
    "\n",
    "        if os.path.exists(OUTPUT_TIF_PATH):\n",
    "            reproject_raster(OUTPUT_TIF_PATH, OUTPUT_TIF_PATH)\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping final tile merging due to errors or no tiles found during debuffering.\")\n",
    "    else:\n",
    "        print(\"Skipping debuffering and merging steps due to errors during the initial tiling and rasterization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144162e",
   "metadata": {},
   "source": [
    "## 2.4. Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13bb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from scipy.interpolate import griddata\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "INPUT_TO_INTERPOLATE = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/LiDAR/tiles_debuffered/atlanta_final_merged_dem.tif\"\n",
    "INTERPOLATED_OUTPUT_PATH = \"/media/remap/NO_HEAT_RB/City_Atlanta/Processed/DSM/atlanta_DSM.tif\"\n",
    "\n",
    "# Parameter\n",
    "INVALID_NUM = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bf38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_window(\n",
    "    ji_window: Tuple[int, Window],\n",
    "    input_path: str,\n",
    "    no_data_value: float,\n",
    "    method: str,\n",
    "    fill_value: float,\n",
    "    buffer: int,\n",
    "    invalid_lt: float\n",
    ") -> Tuple[int, Window, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Interpolates invalid data within a single window of a raster.\n",
    "\n",
    "    This function is designed to be called by a multiprocessing Pool. It reads\n",
    "    a buffered window, identifies valid and invalid pixels, and uses vectorized\n",
    "    `griddata` to perform interpolation.\n",
    "\n",
    "    Args:\n",
    "        ji_window: A tuple containing the window index and a rasterio Window object.\n",
    "        input_path: Path to the source raster file.\n",
    "        no_data_value: The value representing no data in the raster.\n",
    "        method: Interpolation method to use ('linear', 'nearest', 'cubic').\n",
    "        fill_value: The value to use for pixels that cannot be interpolated.\n",
    "        buffer: The buffer size (in pixels) to add around the window to avoid edge effects.\n",
    "        invalid_lt: Values less than this number are considered invalid.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the window index, the original window, and the\n",
    "        interpolated numpy array for that window.\n",
    "    \"\"\"\n",
    "    ji, window = ji_window\n",
    "\n",
    "    # Open the source raster inside each worker process for process safety\n",
    "    with rasterio.open(input_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "\n",
    "        # Create a buffered read-window to avoid edge effects during interpolation\n",
    "        rs = max(0, window.row_off - buffer)\n",
    "        re = min(height, window.row_off + window.height + buffer)\n",
    "        cs = max(0, window.col_off - buffer)\n",
    "        ce = min(width, window.col_off + window.width + buffer)\n",
    "        read_w = Window(cs, rs, ce - cs, re - rs)\n",
    "\n",
    "        data = src.read(1, window=read_w, boundless=True).astype(np.float32)\n",
    "\n",
    "        # Define masks for valid and invalid data points\n",
    "        # Invalid points are no_data, less than a threshold, or NaN.\n",
    "        invalid_mask = (\n",
    "            (data == no_data_value) |\n",
    "            (data < invalid_lt) |\n",
    "            np.isnan(data)\n",
    "        )\n",
    "        valid_mask = ~invalid_mask\n",
    "\n",
    "        # If the buffered window contains no valid data, fill the whole window\n",
    "        # with the fill_value and return.\n",
    "        if not valid_mask.any():\n",
    "            out = np.full((window.height, window.width), fill_value, dtype=np.float32)\n",
    "            return ji, window, out\n",
    "\n",
    "        # Get the coordinates and values of all valid points\n",
    "        rows, cols = np.indices(data.shape)\n",
    "        valid_pts = np.column_stack((rows[valid_mask], cols[valid_mask]))\n",
    "        valid_vals = data[valid_mask]\n",
    "\n",
    "        # Get the coordinates of all invalid points that need to be filled\n",
    "        interp_pts = np.column_stack((rows[invalid_mask], cols[invalid_mask]))\n",
    "        \n",
    "        # Create a copy of the data to hold the interpolated values\n",
    "        filled = data.copy()\n",
    "\n",
    "        # *** CORE OPTIMIZATION ***\n",
    "        # Instead of looping through each point, we pass all points to griddata\n",
    "        # at once. This is massively faster as it uses SciPy's vectorized C-backend.\n",
    "        if interp_pts.size > 0:\n",
    "             interpolated_values = griddata(\n",
    "                points=valid_pts,\n",
    "                values=valid_vals,\n",
    "                xi=interp_pts,\n",
    "                method=method,\n",
    "                fill_value=fill_value\n",
    "            )\n",
    "             filled[invalid_mask] = interpolated_values\n",
    "\n",
    "        # Extract the central, non-buffered part of the processed window\n",
    "        row_offset = window.row_off - rs\n",
    "        col_offset = window.col_off - cs\n",
    "        out = filled[row_offset:row_offset + window.height, col_offset:col_offset + window.width]\n",
    "\n",
    "        return ji, window, out\n",
    "\n",
    "def interpolate_tif_mp(\n",
    "    input_tif: str,\n",
    "    output_tif: str,\n",
    "    invalid_lt: float,\n",
    "    no_data_value: int = -9999,\n",
    "    method: str = 'linear',\n",
    "    fill_value: int = -9999,\n",
    "    search_radius: int = 50,\n",
    "    tile_size: int = 500,\n",
    "    n_workers: int = 4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Interpolates a TIF file using multiprocessing.\n",
    "\n",
    "    Args:\n",
    "        input_tif: Path to the input TIF file.\n",
    "        output_tif: Path for the output interpolated TIF file.\n",
    "        invalid_lt: Values less than this will be treated as invalid.\n",
    "        no_data_value: The no-data value in the source raster.\n",
    "        method: Interpolation method ('linear', 'nearest', 'cubic').\n",
    "        fill_value: Value to fill in where interpolation is not possible.\n",
    "        search_radius: Buffer size around each tile for seamless interpolation.\n",
    "        tile_size: The size of tiles to process in parallel.\n",
    "        n_workers: Number of worker processes to use.\n",
    "    \"\"\"\n",
    "    with rasterio.open(input_tif) as src:\n",
    "        height, width = src.height, src.width\n",
    "        profile = src.profile.copy()\n",
    "        profile.update(dtype=np.float32, nodata=fill_value)\n",
    "\n",
    "        windows = []\n",
    "        idx = 0\n",
    "        for row_off in range(0, height, tile_size):\n",
    "            for col_off in range(0, width, tile_size):\n",
    "                win_h = min(tile_size, height - row_off)\n",
    "                win_w = min(tile_size, width - col_off)\n",
    "                win = Window(col_off, row_off, win_w, win_h)\n",
    "                windows.append((idx, win))\n",
    "                idx += 1\n",
    "        print(f\"Created {len(windows)} windows of up to {tile_size}x{tile_size} pixels.\")\n",
    "\n",
    "    # Use functools.partial to freeze parameters for the worker function\n",
    "    worker_fn = partial(\n",
    "        process_window,\n",
    "        input_path=input_tif,\n",
    "        no_data_value=no_data_value,\n",
    "        method=method,\n",
    "        fill_value=fill_value,\n",
    "        buffer=search_radius,\n",
    "        invalid_lt=invalid_lt,\n",
    "    )\n",
    "\n",
    "    # Open the destination file for writing\n",
    "    with rasterio.open(output_tif, 'w', **profile) as dst:\n",
    "        # Use a multiprocessing Pool\n",
    "        with Pool(n_workers) as pool:\n",
    "            for _, win, data in tqdm(\n",
    "                pool.imap_unordered(worker_fn, windows),\n",
    "                total=len(windows),\n",
    "                desc=\"Interpolating windows\"\n",
    "            ):\n",
    "                dst.write(data.astype(profile['dtype']), 1, window=win)\n",
    "\n",
    "    print(f\"\\nDone. Interpolated file saved to: {output_tif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example of how you would call the function with your original paths\n",
    "    interpolate_tif_mp(\n",
    "        input_tif=INPUT_TO_INTERPOLATE,\n",
    "        output_tif=INTERPOLATED_OUTPUT_PATH,\n",
    "        invalid_lt=INVALID_NUM)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
